# ADR-006: Data Management and Middle Ground Development Strategy

## Status
Accepted

## Context
Need to design data management strategy for the IGDB data pipeline that handles:
- Deduplication of games across multiple ingestion runs
- Incremental updates to avoid re-fetching existing data
- Pipeline state management between ingestion, processing, and training
- Data storage strategy for development and production
- Data quality and integrity across pipeline stages
- **Middle Ground Approach**: Balance development speed with production readiness
- **Scalability**: Develop with small datasets but design for 350k games

## Decision
Implement **SQLite-Only** approach with **Smart Ingestion**, **Pipeline State Tracking**, and **Middle Ground Development Strategy**.

### Core Principles

1. **Middle Ground Development**
   - **Develops with 10-100 games** for rapid iteration and testing
   - **Designs for 350k games** to ensure production scalability
   - **Avoids unnecessary re-fetching** through smart data management
   - **Maintains end-to-end functionality** at all scales

2. **Scalable Pipeline Architecture**
   - Same codebase works for 100 games and 350k games
   - Parameterized limits for different environments
   - Robust error handling and retry logic
   - Memory-efficient batch processing

3. **Development Workflow**
   ```bash
   # Development (100 games)
   python -m data_pipeline.main --limit 100

   # Testing (10k games)
   python -m data_pipeline.main --limit 10000

   # Production (350k games)
   python -m data_pipeline.main --limit 350000
   ```

### Data Storage Strategy
```python
# Development: SQLite
# Production: SQLite + Cloud Storage Backup

class DataPipeline:
    def __init__(self, db_path="data/games.db"):
        self.db = sqlite3.connect(db_path)  # SQLite only
        self.storage = CloudStorage()  # GCP Cloud Storage for backup
```

### Why SQLite-Only?
- **350,000 games** is not "big data" - SQLite handles this easily
- **Simple queries** (filter, sort, search) - SQLite is optimized for this
- **Single-server deployment** - web app and database on same server
- **Cost-effective** - $0 database costs vs $50-200/month for PostgreSQL
- **Development simplicity** - no database server to manage

### Database Schema
```sql
-- Games table with IGDB data
CREATE TABLE games (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    summary TEXT,
    genres JSON,
    platforms JSON,
    themes JSON,
    rating REAL,
    rating_count INTEGER,
    first_release_date INTEGER,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Ingestion tracking
CREATE TABLE ingestion_log (
    id INTEGER PRIMARY KEY,
    batch_id TEXT,
    games_fetched INTEGER,
    games_new INTEGER,
    games_updated INTEGER,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    status TEXT
);

-- Pipeline state tracking
CREATE TABLE processing_status (
    game_id INTEGER,
    feature_extraction_status TEXT,
    model_training_status TEXT,
    last_processed_at TIMESTAMP,
    FOREIGN KEY (game_id) REFERENCES games (id)
);
```

### Smart Ingestion Strategy
```python
def smart_ingest(self, target_count=100):
    # 1. Check current database count
    current_count = self.db.count_games()

    if current_count >= target_count:
        logger.info(f"âœ… Already have {current_count} games, no need to fetch")
        return current_count

    # 2. Calculate how many more games needed
    needed = target_count - current_count
    logger.info(f"ðŸ“¥ Need {needed} more games, fetching from IGDB...")

    # 3. Fetch from IGDB with rate limiting
    games = self.igdb.fetch_games_sample(needed)

    # 4. Save to SQLite (automatic deduplication via PRIMARY KEY)
    self.db.save_games(games)

    # 5. Return new count
    return self.db.count_games()
```

## Rationale

### Why SQLite-Only?
- **Simplicity**: File-based database, no server management
- **Performance**: <1ms queries for 350k games with proper indexing
- **Cost**: $0 database costs vs $50-200/month for PostgreSQL
- **Portability**: Database file can be easily backed up and moved
- **Development**: Perfect for single-developer projects

### Why Middle Ground Approach?
1. **Development Speed**: Working with 100 games allows rapid iteration
2. **Production Readiness**: Same architecture scales to 350k games
3. **Cost Efficiency**: Avoid unnecessary API calls and processing
4. **Data Integrity**: Database constraints prevent duplicates
5. **Flexibility**: Easy to test different scales (100 â†’ 10k â†’ 350k)

### Why Smart Ingestion?
- **Avoid Re-fetching**: Check database count before fetching
- **Rate Limit Compliance**: Respects IGDB's 4 req/s limit
- **Cost Optimization**: Reduces unnecessary API calls
- **Flexibility**: Easy to scale from 100 to 350k games

### Why Pipeline State Tracking?
- **Reliability**: Know which games are processed
- **Resumability**: Restart pipeline from last checkpoint
- **Monitoring**: Track pipeline performance and errors
- **Debugging**: Identify where pipeline fails

## Consequences

### Positive
- âœ… Fast development iteration with small datasets (100 games)
- âœ… Production-ready architecture from day one
- âœ… No unnecessary re-fetching of data
- âœ… Easy scaling from 100 to 350k games
- âœ… Cost-effective development approach
- âœ… Automatic deduplication via PRIMARY KEY constraints
- âœ… Smart ingestion avoids unnecessary re-fetching
- âœ… Reliable pipeline state management
- âœ… Easy data querying and analysis
- âœ… Zero database infrastructure costs
- âœ… Simple backup (copy SQLite file)
- âœ… Perfect for single-server deployment

### Negative
- âŒ Requires database setup (even if lightweight)
- âŒ More complex than simple file storage
- âŒ Limited concurrent write access (not needed for our use case)
- âŒ No network access (not needed - same server as web app)
- âŒ File-based (requires file system access)

### Risks
- **Data Schema Evolution**: Changes to IGDB API might require schema updates
- **Performance at Scale**: Need to monitor performance as dataset grows
- **Concurrent Writes**: SQLite locks database during writes (mitigated by offline ingestion)
- **File Corruption**: SQLite file corruption (mitigated by regular backups)
- **Performance**: Large dataset queries (mitigated by proper indexing)
- **Scaling**: If we need >100 concurrent users (migration path to PostgreSQL available)

## Implementation Plan

### Phase 1: SQLite Database Setup
```python
# Create SQLite database schema
# Implement DataManager class with SQLite
# Add proper indexing for performance
# Test with small dataset (100 games)
```

### Phase 2: Smart Ingestion
```python
# Implement smart ingestion logic
# Add re-fetching avoidance
# Test with multiple ingestion runs
# Validate data integrity and deduplication
```

### Phase 3: Pipeline State Management
```python
# Add processing_status tracking
# Implement checkpoint system
# Add monitoring and logging
# Test pipeline resumability
```

### Phase 4: Production Deployment
```python
# Deploy SQLite database to production
# Add Cloud Storage backup automation
# Implement monitoring and alerting
# Add data quality checks
# Create migration path to PostgreSQL if needed
```

## Data Quality Strategy

### Validation Rules
- **Required Fields**: name, summary, genres, platforms
- **Data Types**: Validate JSON fields, numeric ranges
- **Consistency**: Check genre/platform IDs against reference tables
- **Completeness**: Flag games with missing critical data

### Quality Metrics
- **Coverage**: Percentage of games with complete data
- **Freshness**: Age of data vs IGDB updates
- **Accuracy**: Validation against IGDB reference data
- **Completeness**: Missing field analysis

## Future Considerations
- **Migration Path**: Easy migration to PostgreSQL if scaling needed
- **Real-time Updates**: Webhook integration with IGDB
- **Data Versioning**: Track changes over time
- **Analytics**: Data quality dashboards
- **Backup Strategy**: Automated SQLite file backup to Cloud Storage
- **Performance Optimization**: SQLite indexing and query optimization

## Migration Path to PostgreSQL
If we encounter SQLite limitations:
```python
# Migration script: SQLite â†’ PostgreSQL
def migrate_to_postgresql():
    # 1. Export SQLite data
    sqlite_data = export_sqlite_data()

    # 2. Create PostgreSQL schema
    create_postgresql_schema()

    # 3. Import data to PostgreSQL
    import_to_postgresql(sqlite_data)

    # 4. Update DataManager to use PostgreSQL
    # 5. Deploy with PostgreSQL connection
```

## Success Metrics
- **Development**: Complete pipeline with 100 games in < 1 hour
- **Testing**: Scale to 10k games without code changes
- **Production**: Handle 350k games with acceptable performance
- **Data Integrity**: Zero duplicate games in database
- **Cost**: Minimize unnecessary API calls and processing
- **Deduplication**: 0% duplicate games in database
- **Smart Ingestion**: <5% redundant API calls
- **Pipeline Reliability**: >99% successful ingestion runs
- **Data Quality**: >95% games with complete required fields
- **Performance**: <1ms for simple queries, <100ms for complex queries
- **Cost**: $0 database costs (vs $50-200/month for PostgreSQL)
